---
layout: null
section-type: research
title: Research
---
## My Research

My research interests lie at the intersection of Computer Vision and Machine Learning, with a focus on building algorithms that can:

<ul>
    <li> explain why they believe what they believe </li>
    <li> truly understand and grasp information from different modalities (vision, language, common sense reasoning) and create other plausible scenarios </li>
</ul>

### Projects

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	<tbody><tr>
		<td valign="top" width="30%">
			 <img src="teaser_behavioral-analysis.png" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			<b>Analyzing the Behavior of Visual Question Answering Models</b>
			<br> 
			Aishwarya Agrawal, 
			<a href="https://computing.ece.vt.edu/~dbatra/">Dhruv Batra</a>, 
			<a href="https://computing.ece.vt.edu/~parikh/">Devi Parikh</a>
			<br>
			arXiv preprint 	arXiv:1606.07356, 2016
			<br>
			[<a href="https://arxiv.org/pdf/1606.07356v1.pdf">Paper</a>]
		</td>
	</tr>
	
	<tr>
		<td valign="top" width="30%">
			 <img src="teaser_ai-mag.png" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			<b>Measuring Machine Intelligence Through Visual Question Answering</b>
			<br>
			<a href="http://research.microsoft.com/en-us/people/larryz/">C. Lawrence Zitnick</a>, 
			Aishwarya Agrawal, 
			<a href="https://computing.ece.vt.edu/~santol/">Stanislaw Antol</a>, 
			<a href="http://www.m-mitchell.com/">Margaret Mitchell</a>, 
			<a href="https://computing.ece.vt.edu/~dbatra/">Dhruv Batra</a>, 
			<a href="https://computing.ece.vt.edu/~parikh/">Devi Parikh</a>
			<br>
			AI Magazine, 2016
			<br>
			[<a href="http://www.aaai.org/ojs/index.php/aimagazine/article/view/2647">Paper</a>]
		</td>
	</tr>
	 
	<tr>
		<td valign="top" width="30%">
		     <img src="teaser_holistic.PNG" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			 <b>Resolving Language and Vision Ambiguities Together: Joint Segmentation &amp; Prepositional Attachment Resolution in Captioned Scenes</b>
			 <br>
			 Gordon Christie*, 
			 Ankit Laddha*, 
			 Aishwarya Agrawal, 
			 <a href="https://computing.ece.vt.edu/~santol/">Stanislaw Antol, 
			 </a><a href="https://computing.ece.vt.edu/~ygoyal/">Yash Goyal</a>, 
			 <a href="http://www.me.vt.edu/people/faculty/kevin-kochersberger/">Kevin Kochersberger</a>, 
			 <a href="https://filebox.ece.vt.edu/~dbatra/">Dhruv Batra</a>
			 <br>
			 <p style="line-height: 150%; margin-top: 0pt; margin-bottom: 0pt;">*equal contribution</p>
			 arXiv preprint arXiv:1604.02125, 2016
			 <br>
			 [<a href="http://arxiv.org/pdf/1604.02125v1.pdf">Paper</a>]
		</td>
	</tr>
	
	<tr>
		<td valign="top" width="30%">
			 <img src="teaser_visual-storytelling.png" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			 <b>Visual Storytelling</b>
			 <br>
			 <a href="http://www.cs.cmu.edu/%7Etinghaoh/">Ting-Hao Huang</a>, 
			 <a href="http://cs.jhu.edu/%7Eferraro/">Francis Ferraro</a>, 
			 <a href="http://www.cs.rochester.edu/%7Enasrinm/">Nasrin Mostafazadeh</a>, 
			 <a href="http://www.cs.cmu.edu/%7Eimisra/">Ishan Misra</a>, 
			 Aishwarya Agrawal,
			 <a href="https://www.linkedin.com/in/jacob-devlin-135ab048">Jacob Devlin</a>
			 <a href="http://www.cs.berkeley.edu/%7Erbg/">Ross Girshick</a>, 
			 <a href="http://research.microsoft.com/en-us/people/xiaohe/">Xiaodong He</a>, 
			 <a href="http://research.microsoft.com/en-us/um/people/pkohli/">Pushmeet Kohli</a>, 
			 <a href="https://filebox.ece.vt.edu/%7Edbatra/">Dhruv Batra</a>,
			 <a href="http://larryzitnick.org/">C. Lawrence Zitnick</a>, 
			 <a href="https://computing.ece.vt.edu/~parikh/">D. Parikh</a>, 
			 <a href="http://research.microsoft.com/en-us/people/lucyv/">Lucy Vanderwende</a>, 
			 <a href="http://research.microsoft.com/en-us/people/mgalley/">Michel Galley</a>, 
			 <a href="http://www.m-mitchell.com/">Margaret Mitchell</a> 
			 <br>
			Conference of the North American Chapter of the Association for Computational Linguistics: 
			Human Language Technologies (NAACL HLT), 2016
			<br>
			[<a href="https://arxiv.org/pdf/1604.03968v1.pdf">Arxiv Paper</a>, <a href="http://www.sind.ai/">Project Page</a>]
		</td>
	</tr>

	<tr>
		<td valign="top" width="30%">
			 <img src="teaser_v2.png" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			 <b>VQA: Visual Question Answering</b>
			 <br>
			 <a href="https://computing.ece.vt.edu/~santol/">Stanislaw Antol</a>*, 
			 Aishwarya Agrawal*, 
			 <a href="https://filebox.ece.vt.edu/~jiasenlu/">Jiasen Lu</a>,
			 <a href="http://research.microsoft.com/en-us/people/memitc/">Margaret Mitchell</a>, 
			 <a href="https://computing.ece.vt.edu/~dbatra/">Dhruv Batra</a>, 
			 <a href="http://research.microsoft.com/en-us/people/larryz/">C. Lawrence Zitnick</a>, 
			 <a href="https://computing.ece.vt.edu/~parikh/">Devi Parikh</a>
			 <br>
			 <p style="line-height: 150%; margin-top: 0pt; margin-bottom: 0pt;">*equal contribution</p>
			 International Conference on Computer Vision (ICCV), 2015
			 <br>
			 [ <a href="VQA_ICCV2015.pdf">ICCV Camera Ready Paper</a>
			 | <a href="http://arxiv.org/abs/1505.00468">arxiv</a>
			 | <a href="https://www.youtube.com/embed/gar88i-V0RE">ICCV Spotlight</a>
			 | <a href="http://visualqa.org">visualqa.org (data, code, challenge)</a>
			 | <a href="VQA_AishwaryaAgrawal_GTC2016.pptx">slides</a>
			 | <a href="http://on-demand.gputechconf.com/gtc/2016/video/S6745.html">talk at GPU Technology Conference (GTC) 2016</a>]
		</td>
	</tr>

	<tr>
		<td valign="top" width="30%">
			 <img src="teaser_tmo_final.png" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			  <b>A Novel LBP Based Operator for Tone Mapping HDR Images</b>
			  <br>
			  Aishwarya Agrawal, 
			  <a href="https://sites.google.com/a/iitgn.ac.in/shanmuga/">Shanmuganathan Raman</a>
			  <br>
			  International Conference on Signal Processing and Communications (SPCOM-2014)
			  <br>
			  [<a href="spcom2014.pdf">Paper</a>
			  |<a href="poster.pdf">Poster</a>]
		</td>
	</tr>
	
	<tr>
		<td valign="top" width="30%">
			 <img src="teaser_tissue.png" width="250" height="auto">
		</td>
		<td valign="top" width="70%">
			  <b>Optically clearing tissue as an initial step for 3D imaging of core biopsies to diagnose pancreatic cancer</b>
			  <br>
			  <a href="http://staff.washington.edu/rdas/">Ronnie Das</a>, 
			  Aishwarya Agrawal, 
			  <a href="http://www.pathology.washington.edu/faculty/upton">Melissa P. Upton</a>, 
			  <a href="https://www.me.washington.edu/research/faculty/eseibel/index.html">Eric J. Seibel</a>
			  <br>
			  SPIE BiOS, International Society for Optics and Photonics, 2014
			  <br>
			  [<a href="spie2014">Paper</a>]
		</td>
	</tr>
</tbody></table>




#### Making Deep Models Interpretable Without Making Interpretable Deep Models

<p> Deep CNNs have enabled significant breakthroughs for vision, but at the same time they raise important questions about interpretability. To make these models more interpretable we start by (1) proposing a novel class localization method called Gradient-weighted Class Activation Maps (Grad-CAM) which uses gradients to perform localization. (2) Then we combine Grad-CAM with visualizations like Guided Backpropagation to create a novel visualization method called Guided Grad-CAM. The resulting visualizations provide intuitive explanations for CNN predictions by showing both what salient features a CNN looks at as well as where it looks. (3) Further, we empirically demonstrate that our visualization technique is highly class-discriminative through human studies. (4) Finally, we show the broad applicability of this visualization technique not just for image classification, but also visual question answering and image captioning, showing how to interpret typically uninterpretable deep models. </p>
<p> Arxiv link and Code coming out soon </p>

#### Counting Everyday Objects in Everyday Scenes 

<p> We introduce the problem of counting everyday objects in everyday scenes. While previous works have studied specific counting problems such as pedestrian counting in surveillance videos, or biological cell counting, we are interested in counting common objects in natural scenes. We study this problem in a setup similar to  traditional scene understanding problems. Given an image, we consider the task of predicting the counts (or the numerosity) of categories of interest. We study some
simple approaches and applications for this counting problem. Our detect approach adapts an object detector to perform counting, while our glance approach regresses to ground truth counts. Our associative subitizing (aso-sub) approach divides an image into regions and regresses to fractional object counts in each region. We create an ensemble (ens) of these counting methods which improves performance. We demonstrate counting performance on the PASCAL and MS COCO datasets. We show proof-of-concept applications of our  automatic counting methods to 1) improve object detection performance, and 2) visual question answering (on VQA and COCO-QA). </p>
<p> Arxiv: </p>
<p> Code: </p>

#### Diverse Beam Search: Diverse Decoding from Neural Sequence Models 

<p> The use of neural sequence models has become standard practice for modeling time-series data in a wide variety of fields. Equally ubiquitous is the use of beam search as an approximate inference algorithm to produce high probability outputs from these models. Beam Search explores the search-space in a best-first fashion, retaining only a small set of highly likely candidate solutions; however, these candidates are often minor perturbations of each other. Producing lists of nearly identical sequences is computationally wasteful and fails to capture multi-modal beliefs. In this work, we present Diverse Beam Search, a broadly-applicable beam search replacement that produces a list of diverse beams and addresses these short-comings. Our results show that diverse beam search not only produces outputs that are significantly more varied but also tends to find better solutions overall as diversity encourages greater exploration of the search space. We present results using both standard quantitative metrics and qualitative human studies. </p>

#### The Semantic Paintbrush: Interactive 3D Mapping and Recognition in Large Outdoor Spaces

<p> We present an augmented reality system for large scale 3D reconstruction and recognition in outdoor scenes. Unlike existing prior work, which tries to reconstruct scenes using active depth cameras, we use a purely passive stereo setup, allowing for outdoor use and extended sensing range. Our system not only produces a map of the 3D environment in real-time, it also allows the user to draw (or ‘paint’) with a laser pointer directly onto the reconstruction to segment the model into objects. Given these examples our system then learns to segment other parts of the 3D map during online acquisition. Unlike typical object recognition systems, ours therefore very much places the user ‘in the loop’ to segment particular objects of interest, rather than learning from predefined databases. The laser pointer additionally helps to ‘clean up’ the stereo reconstruction and final 3D map, interactively. Using our system, within minutes, a user can capture a full 3D map, segment it into objects of interest, and refine parts of the model during capture. We provide full technical details of our system to aid replication, as well as quantitative evaluation of system components. We demonstrate the possibility of using our system for helping the visually impaired navigate through spaces. Beyond this use, our system can be used for playing large-scale augmented reality games, shared online to augment streetview data, and used for more detailed car and person navigation. </p>

#### Blind-Find: Wearable Computer-Vision Rig for the Visually Impaired

<p> For visually impaired individuals the navigation of unfamiliar environments can be both time consuming and hazardous, often requiring the help of a sighted friend. The recent advances in Global Positioning Systems has allowed for novel methods of outdoor navigation, both among the visually impaired and sighted populations. However, in GPS denied environments, such as indoor locations,
there is currently a lack of navigational aid. This research addresses this problem through the construction of a wearable computer system which utilizes both inertial and visual odometry systems, as well as comprehensive image matching and feedback to guide visually impaired users through a variety of indoor environments. </p>



